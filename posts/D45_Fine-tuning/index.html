

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="mztchaoqun">
  <meta name="keywords" content="hexo,theme,fluid,material,material-design,blog">
  
    <meta name="description" content="随着模型变得越来越大，在消费级硬件上对模型进行全部参数的微调变得不可行。此外，为每个下游任务独立存储和部署微调模型变得非常昂贵，因为微调模型与原始预训练模型的大小相同。参数高效微调(PEFT) 方法旨在解决这两个问题！PEFT 方法使您能够获得与全参数微调相当的性能，同时只有少量可训练参数。 一、PEFT类型  Additive methods Additive methods的主要思想是通过添加">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM(七)——参数高效微调（Parameter-efficient fine-tuning,PEFT）">
<meta property="og:url" content="https://mztchaoqun.com.cn/posts/D45_Fine-tuning/index.html">
<meta property="og:site_name" content="Suny的文章">
<meta property="og:description" content="随着模型变得越来越大，在消费级硬件上对模型进行全部参数的微调变得不可行。此外，为每个下游任务独立存储和部署微调模型变得非常昂贵，因为微调模型与原始预训练模型的大小相同。参数高效微调(PEFT) 方法旨在解决这两个问题！PEFT 方法使您能够获得与全参数微调相当的性能，同时只有少量可训练参数。 一、PEFT类型  Additive methods Additive methods的主要思想是通过添加">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mztchaoqun.com.cn/images/PEFT.webp">
<meta property="article:published_time" content="2024-11-02T10:47:32.000Z">
<meta property="article:modified_time" content="2026-02-27T13:41:45.274Z">
<meta property="article:author" content="mztchaoqun">
<meta property="article:tag" content="Fine-tuning">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="PEFT">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://mztchaoqun.com.cn/images/PEFT.webp">
  
  
  
  <title>LLM(七)——参数高效微调（Parameter-efficient fine-tuning,PEFT） - Suny的文章</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"mztchaoqun.com.cn","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 8.1.1"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Suny的文章</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/it-tools/" target="_self">
                <i class="iconfont icon-briefcase"></i>
                <span>it-tools</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-books"></i>
                <span>文档</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="https://hexo.fluid-dev.com/docs/start/" target="_self">
                    
                    <span>安装主题</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://hexo.fluid-dev.com/docs/guide/" target="_self">
                    
                    <span>配置指南</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="https://hexo.fluid-dev.com/docs/icon/" target="_self">
                    
                    <span>图标用法</span>
                  </a>
                
              </div>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/images/post_banner.webp') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="LLM(七)——参数高效微调（Parameter-efficient fine-tuning,PEFT）"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-11-02 18:47" pubdate>
          2024年11月2日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.8k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          49 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">LLM(七)——参数高效微调（Parameter-efficient fine-tuning,PEFT）</h1>
            
            
              <div class="markdown-body">
                
                <p>随着模型变得越来越大，在消费级硬件上对模型进行全部参数的微调变得不可行。此外，为每个下游任务独立存储和部署微调模型变得非常昂贵，因为微调模型与原始预训练模型的大小相同。参数高效微调(PEFT)
方法旨在解决这两个问题！PEFT
方法使您能够获得与全参数微调相当的性能，同时只有少量可训练参数。</p>
<h2 id="一peft类型">一、PEFT类型</h2>
<p><img src="/images/PEFT.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>Additive methods</strong></p>
<p>Additive
methods的主要思想是通过添加额外的参数或层来扩充现有的Pre-Trained
models，仅对新添加的参数进行训练。主要有两个类别：Adapter和Soft
Prompt。</p>
<p>Adapter是在Transformer架构中引入小的全连接可训练层。而Soft
Prompt旨在通过保持其结构固定和冻结来修改输入Prompt，从而控制LLM的行为。</p>
<p><strong>Selective Methods</strong></p>
<p>选择性方法对模型现有的参数进行微调，可以是基于层深度的选择、基于层类型的选择，甚至是个别参数的选择。其中一个例子是注意力调整。这些基于选择性的方法的性能有好有坏，并且在参数效率和计算效率之间存在明显的折中。</p>
<p><strong>Reparametrization-based methods</strong></p>
<p>基于重新参数化的PEFT方法利用Low-Rank
Approximation性质来最小化可训练参数的数量。Low-Rank
Matrix旨在捕捉高维数据的潜在Low-Rank结构。该方法冻结原始LLM参数，通过建立新的Low-Rank转换并引入少量可训练参数。</p>
<h3 id="代表性peft方法">1.1 代表性PEFT方法</h3>
<h4 id="prompt-tuning">1.1.1 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08691">Prompt Tuning</a></h4>
<ul>
<li>在输入序列前，额外加入一组伪 Embedding 向量</li>
<li>只训练这组伪 Embedding，从而达到参数微调的效果</li>
</ul>
<p><img src="/images/soft-prompt.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="p-tuning">1.1.2 P-Tuning</h4>
<ul>
<li>用一个生成器生成上述伪 Embedding</li>
<li>只有生成器的参数是可训练的</li>
</ul>
<p><img src="/images/pt.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="prefix-tuning">1.1.3 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.00190">Prefix-Tuning</a></h4>
<ul>
<li>伪造前面的 Hidden States</li>
<li>只训练伪造的这个 Prefix</li>
</ul>
<p><img src="/images/pt2.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="lora">1.1.4 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.09685">LoRA</a></h4>
<ul>
<li>在 Transformer 的参数矩阵上加一个低秩矩阵（<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="6.18ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 2731.4 716"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(972.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1972.4,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g></g></svg></mjx-container></span>）</li>
<li>只训练 A，B</li>
<li>理论上可以把上述方法应用于 Transformer 中的任意参数矩阵，包括
Embedding 矩阵</li>
<li>通常应用于 Query, Value 两个参数矩阵</li>
</ul>
<p><img src="/images/lora.png" srcset="/img/loading.gif" lazyload></p>
<p>对于预训练权重 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="10.619ex" height="2.306ex" role="img" focusable="false" viewBox="0 -853.7 4693.6 1019.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(1658.3,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2603.1,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(520,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1298,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></g></svg></mjx-container></span>，将计算隐藏层数值的公式<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="8.738ex" height="1.945ex" role="img" focusable="false" viewBox="0 -694 3862.1 859.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(853.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(1909.6,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mi" transform="translate(3290.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container></span>修改成</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -3.681ex;" xmlns="http://www.w3.org/2000/svg" width="22.143ex" height="8.493ex" role="img" focusable="false" viewBox="0 -2126.9 9787.2 3753.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0,1376.9)"><g data-mml-node="mtd" transform="translate(1488,0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g><g data-mml-node="mtd" transform="translate(2064,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(1333.6,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mi" transform="translate(2714.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3508.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(4508.6,0)"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g><g data-mml-node="mi" transform="translate(4952.6,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(6000.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g><g data-mml-node="mtr" transform="translate(0,76.9)"><g data-mml-node="mtd"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g><g data-mml-node="mi" transform="translate(444,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1492,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g><g data-mml-node="mtd" transform="translate(2064,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1333.6,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(2092.6,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(2842.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g><g data-mml-node="mtr" transform="translate(0,-1376.9)"><g data-mml-node="mtd" transform="translate(1305,0)"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g><g data-mml-node="mtd" transform="translate(2064,0)"><g data-mml-node="mi"></g><g data-mml-node="mo" transform="translate(277.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(1222.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(520,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1298,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(3264.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3709,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(4736.7,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(5681.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(451,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1229,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></g></g></g></g></svg></mjx-container></span></p>
<p>使用低秩矩阵分解(<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="3.414ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 1509 716"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(759,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g></svg></mjx-container></span>)来限制更新，并且<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="18.539ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 8194.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(451,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(980,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1580,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mi" transform="translate(2101,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2829.8,0)"><path data-c="226A" d="M639 -48Q639 -54 634 -60T619 -67H618Q612 -67 536 -26Q430 33 329 88Q61 235 59 239Q56 243 56 250T59 261Q62 266 336 415T615 567L619 568Q622 567 625 567Q639 562 639 548Q639 540 633 534Q632 532 374 391L117 250L374 109Q632 -32 633 -34Q639 -40 639 -48ZM944 -48Q944 -54 939 -60T924 -67H923Q917 -67 841 -26Q735 33 634 88Q366 235 364 239Q361 243 361 250T364 261Q367 266 641 415T920 567L924 568Q927 567 930 567Q944 562 944 548Q944 540 938 534Q937 532 679 391L422 250L679 109Q937 -32 938 -34Q944 -40 944 -48Z"></path></g><g data-mml-node="mi" transform="translate(4107.6,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4985.6,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5330.6,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(5930.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6319.6,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(6839.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(7284.2,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(7805.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>。<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.92ex" role="img" focusable="false" viewBox="0 -683 1380.6 848.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container></span>和<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="13.721ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 6064.6 798"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(520,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(986,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1284,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1645,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(2174,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(3499.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4555.6,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(5314.6,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g></svg></mjx-container></span>都与相同的输入相乘。在训练过程中<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.123ex" height="1.92ex" role="img" focusable="false" viewBox="0 -683 1380.6 848.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container></span>被冻结并且不接收梯度更新。</p>
<p><strong>QLoRA</strong></p>
<p>什么是模型量化</p>
<p><img src="/images/float.png" srcset="/img/loading.gif" lazyload> <img src="/images/quant.png" srcset="/img/loading.gif" lazyload></p>
<p>更多参考: <a target="_blank" rel="noopener" href="https://huggingface.co/blog/hf-bitsandbytes-integration">https://huggingface.co/blog/hf-bitsandbytes-integration</a></p>
<p>QLoRA 引入了许多创新来在不牺牲性能的情况下节省显存：</p>
<ul>
<li>4 位
NormalFloat（NF4），一种对于正态分布权重而言信息理论上最优的新数据类型</li>
<li>双重量化，通过量化量化常数来减少平均内存占用</li>
<li>分页优化器，用于管理内存峰值</li>
</ul>
<h4 id="adapter">1.1.5 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08154">Adapter</a></h4>
<p><img src="/images/Adapter.png" srcset="/img/loading.gif" lazyload></p>
<p>Adapter层是加在两个前馈层后面的，所以Adapter方法需要微调的参数就是新增的Adapter层，微调参数量会随着transformer的层数线性增长。</p>
<p>Adapter层是一个瓶颈结构，先把原始的<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container></span>维特征投影到一个更小的维度<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.986ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 878 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>，之后应用非线性函数，最后再次投影回<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container></span>维。包括偏置在内，每层添加的总参数数量是<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="12.988ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 5740.9 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1378,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(2120.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3120.4,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(3862.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(4862.9,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container></span>。通过设置<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.152ex;" xmlns="http://www.w3.org/2000/svg" width="6.682ex" height="1.722ex" role="img" focusable="false" viewBox="0 -694 2953.6 761"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1155.8,0)"><path data-c="226A" d="M639 -48Q639 -54 634 -60T619 -67H618Q612 -67 536 -26Q430 33 329 88Q61 235 59 239Q56 243 56 250T59 261Q62 266 336 415T615 567L619 568Q622 567 625 567Q639 562 639 548Q639 540 633 534Q632 532 374 391L117 250L374 109Q632 -32 633 -34Q639 -40 639 -48ZM944 -48Q944 -54 939 -60T924 -67H923Q917 -67 841 -26Q735 33 634 88Q366 235 364 239Q361 243 361 250T364 261Q367 266 641 415T920 567L924 568Q927 567 930 567Q944 562 944 548Q944 540 938 534Q937 532 679 391L422 250L679 109Q937 -32 938 -34Q944 -40 944 -48Z"></path></g><g data-mml-node="mi" transform="translate(2433.6,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container></span>来保证每个Adapter层的参数量不会很大。</p>
<p>注意到Adapter层也是一个残差结构，所以只要把主路径上的全连接层参数初始化为0，Adapter在一开始的时候就相当于一个全等变换，这样相当于啥也没加，网络应该可以train得起来。</p>
<p><img src="/images/adapter.webp" srcset="/img/loading.gif" lazyload></p>
<p>Adapter和微调最后几层transformer的效果，发现Adapter在相同参数量的情况下基本能够超过后者，在参数量小两个数据级的时候也能达到接近的效果，这说明Adapter在少参数微调的时候还是比较有性价比的。</p>
<h4 id="ia3">1.1.6 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.05638">IA3</a></h4>
<p><img src="/images/IA3.png" srcset="/img/loading.gif" lazyload></p>
<p>具体来说作者修改了Attention，对其加入<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.695ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 749.4 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(331,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></svg></mjx-container></span>和<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.638ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 723.9 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(331,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g></g></g></svg></mjx-container></span></p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.308ex;" xmlns="http://www.w3.org/2000/svg" width="28.548ex" height="5.818ex" role="img" focusable="false" viewBox="0 -1551.7 12618 2571.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(954,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(1504,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1865,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2743,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(3272,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3844,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mfrac" transform="translate(4233,0)"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(791,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1180,0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(331,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(2151.6,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msup" transform="translate(2651.8,0)"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(974,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mo" transform="translate(4173.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="msqrt" transform="translate(1589.1,-855.6)"><g transform="translate(853,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,35.6)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="971.4" height="60" x="853" y="775.6"></rect></g><rect width="4762.7" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(9235.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(9624.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(10013.7,0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(331,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g><g data-mml-node="mo" transform="translate(10959.8,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(11460,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(12229,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></p>
<p>然後在 position-wise feed-forward networks 加入 <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="2.622ex" height="2.237ex" role="img" focusable="false" viewBox="0 -694 1158.8 989"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="TeXAtom" transform="translate(331,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(550,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g></g></g></g></svg></mjx-container></span></p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.667ex;" xmlns="http://www.w3.org/2000/svg" width="16.546ex" height="2.364ex" role="img" focusable="false" viewBox="0 -750 7313.4 1045"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="TeXAtom" transform="translate(331,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(550,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1770,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(2270.3,0)"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g><g data-mml-node="mo" transform="translate(2813.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3202.3,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mi" transform="translate(4582.8,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(5154.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5543.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="msub" transform="translate(5932.8,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container></span></p>
<p><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g></g></g></svg></mjx-container></span>是网络中的非线性层。作者對每一個
Transformer layer 都做了一樣的改動。 ​ <img src="/images/IA3_Loss.png" srcset="/img/loading.gif" lazyload></p>
<p>在few-shot训练集上，IA3用少量参数赢过其他微调方法，并且比Full
Fine-Tuning还要好。</p>
<h4 id="ladder-side-tuning">1.1.7 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.06522">Ladder Side-Tuning</a></h4>
<p><img src="/images/Ladder_Side_Tuning.png" srcset="/img/loading.gif" lazyload></p>
<p>反向传播，也就是求模型梯度，是从输出层向输入层逐步计算的，因此反向传播的深度/计算量，取决于最靠近输入层的参数深度，跟可训练的参数量没有太必然的联系。对于Adapter来说，它在每一层后面都插入了一个小规模的层，虽然其余参数都固定了，只有新插入的层可训练，但每一层都新层，所以反向传播要传到输入层；对于P-tuning来说，本质上它是只有在Embedding层中有少量可训练参数，但Embedding层是输入层，因此它的反向传播也要贯穿整个模型。因此，这两种方案能提升的训练效率并不多。</p>
<p>至于LST，它是在原有大模型的基础上搭建了一个“旁支”（梯子），将大模型的部分层输出作为旁枝模型的输入，所有的训练参数尽在旁枝模型中，由于大模型仅提供输入，因此反向传播的复杂度取决于旁枝模型的规模，并不需要直接在原始大模型上执行反向传播，因此是可以明显提升训练效率的。</p>
<h4 id="bitfit">1.1.8 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.10199">BitFit</a></h4>
<p>BitFit仅微调网络的Bias。BitFit仅更新模型约0.05%的参数量。</p>
<p><img src="/images/BitFit.png" srcset="/img/loading.gif" lazyload></p>
<p>BitFit是一个非常简单的方法，但性能表现比Full Fine-Tuning差。</p>
<h3 id="peft方法的架构和修改位置">1.2 PEFT方法的架构和修改位置</h3>
<p><img src="/images/PEFT_Layer.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="二chatglm3-prefix-tunning">二、ChatGLM3 Prefix-Tunning</h2>
<p>基于ChatGLM3微调一个同时具有 NLU 和问答能力对话机器人</p>
<h3 id="数据源">2.1 数据源</h3>
<p>酒店预订场景：<a target="_blank" rel="noopener" href="https://github.com/thu-coai/CrossWOZ">https://github.com/thu-coai/CrossWOZ</a></p>
<p>酒店数据库：<a target="_blank" rel="noopener" href="https://github.com/thu-coai/CrossWOZ/blob/master/data/crosswoz/database/hotel_db.json">https://github.com/thu-coai/CrossWOZ/blob/master/data/crosswoz/database/hotel_db.json</a></p>
<h3 id="数据增强">2.2 数据增强</h3>
<ul>
<li>从 CrossWOZ 数据集中抽取了只关于酒店的对话</li>
<li>利用 ChatGPT 进行如下修改和补充
<ul>
<li>对设施的描述更口语化
<ul>
<li>“找一家有国际长途电话的酒店” -&gt; “找一家能打国际长途的酒店”</li>
</ul></li>
<li>补充一定比例的多轮问答，和结束语对话（p=0.3）
<ul>
<li>针对只提及一个酒店时的问答：“这个酒店的电话是多少”</li>
<li>针对推荐多个酒店时的对比问答：“哪个酒店评分更高”</li>
<li>结束语：“好的，祝您入住愉快”</li>
</ul></li>
<li>补充按酒店名（简称）、价格上限查询的对话（原数据中没有这类说法）</li>
</ul></li>
</ul>
<p>最终按 8:1:1 拆分训练集、验证集和测试</p>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-65ad56e5" role="button" aria-expanded="false" aria-controls="collapse-65ad56e5">
        <div class="fold-arrow">▶</div>原始样本
      </div>
      <div class="fold-collapse collapse" id="collapse-65ad56e5">
        <div class="fold-content">
          <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"user"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"你好，我出差想去酒店住宿。请帮我推荐一家公共区域和部分房间提供wifi服务，评分是4分以上的酒店。"</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"search"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"arguments"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><br>      <span class="hljs-attr">"facilities"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">"公共区域和部分房间提供wifi"</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">"rating_range_lower"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4.0</span><br>    <span class="hljs-punctuation">}</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"return"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"records"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>      <span class="hljs-punctuation">{</span><br>        <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"北京龙鼎华鼎云酒店"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"舒适型"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"address"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"北京朝阳区潘家园东里18号"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"subway"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"劲松地铁站D口"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"phone"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"010-52001188"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"facilities"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>          <span class="hljs-string">"公共区域和部分房间提供wifi"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"宽带上网"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"国际长途电话"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"吹风机"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"24小时热水"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"中式餐厅"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"会议室"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"无烟房"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"商务中心"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"洗衣服务"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"行李寄存"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"叫醒服务"</span><br>        <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"price"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">-1.0</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"rating"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4.3</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"hotel_id"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">24</span><br>      <span class="hljs-punctuation">}</span><br>    <span class="hljs-punctuation">]</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"assistant"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"那您去北京龙鼎华鼎云酒店住宿吧，酒店质量很好。"</span><br>  <span class="hljs-punctuation">}</span><br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-b870a72b" role="button" aria-expanded="false" aria-controls="collapse-b870a72b">
        <div class="fold-arrow">▶</div>增强后样本
      </div>
      <div class="fold-collapse collapse" id="collapse-b870a72b">
        <div class="fold-content">
          <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"user"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"你好，我出差想去酒店住宿。请帮我推荐一家提供无线网络且评分在4分以上的酒店。"</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"search"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"arguments"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><br>      <span class="hljs-attr">"facilities"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">"无线网络"</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">"rating_range_lower"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4.0</span><br>    <span class="hljs-punctuation">}</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"return"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"records"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>      <span class="hljs-punctuation">{</span><br>        <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"北京龙鼎华鼎云酒店"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"舒适型"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"address"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"北京朝阳区潘家园东里18号"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"subway"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"劲松地铁站D口"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"phone"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"010-52001188"</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"facilities"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>          <span class="hljs-string">"公共区域和部分房间提供wifi"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"宽带上网"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"国际长途电话"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"吹风机"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"24小时热水"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"中式餐厅"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"会议室"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"无烟房"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"商务中心"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"洗衣服务"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"行李寄存"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-string">"叫醒服务"</span><br>        <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"price"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">-1.0</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"rating"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4.3</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"hotel_id"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">24</span><br>      <span class="hljs-punctuation">}</span><br>    <span class="hljs-punctuation">]</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"assistant"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"那您去北京龙鼎华鼎云酒店住宿吧，酒店质量很好。"</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"user"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"这个酒店的评分是多少？"</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"assistant"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"这个酒店的评分是4.3分。"</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"user"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"好的，我决定入住北京龙鼎华鼎云酒店了。"</span><br>  <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"assistant"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"好的，祝您入住愉快。"</span><br>  <span class="hljs-punctuation">}</span><br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>
<h3 id="数据处理">2.3 数据处理</h3>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-cd039fec" role="button" aria-expanded="false" aria-controls="collapse-cd039fec">
        <div class="fold-arrow">▶</div>前置依赖代码
      </div>
      <div class="fold-collapse collapse" id="collapse-cd039fec">
        <div class="fold-content">
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> ast<br><span class="hljs-keyword">import</span> astunparse<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizer<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> copy <span class="hljs-keyword">import</span> deepcopy<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Dict</span>, <span class="hljs-type">List</span><br><br><span class="hljs-comment"># text constants</span><br>FUNCTION_CALL_NAME     = <span class="hljs-string">'tool_call'</span><br>FUNCTION_CALL_PREFIX   = <span class="hljs-string">'```python\n'</span><br>FUNCTION_CALL_POSTFIX  = <span class="hljs-string">'\n```'</span><br>TOOL_DEFINITION_PREFIX = <span class="hljs-string">'Answer the following questions as best as you can. You have access to the following tools:\n'</span><br>CONVERSATOIN_KEY       = <span class="hljs-string">'conversations'</span><br>TOOL_DESC_KEY          = <span class="hljs-string">'tools'</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">format_function_call</span>(<span class="hljs-params">function_name: <span class="hljs-built_in">str</span>, parameters: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]</span>):<br>    function_name = ast.Name(<span class="hljs-built_in">id</span>=function_name)<br>    keywords = [<br>        ast.keyword(arg=arg_name, value=ast.Constant(arg_value)) <br>        <span class="hljs-keyword">for</span> arg_name, arg_value <span class="hljs-keyword">in</span> parameters.items()<br>    ]<br>    func_call = ast.Call(func=function_name, args=[], keywords=keywords)<br>    <span class="hljs-keyword">return</span> astunparse.unparse(func_call).strip()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sanity_check</span>(<span class="hljs-params">tokens: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], tokenizer: PreTrainedTokenizer</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Sanity Check &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"</span>)<br>    <span class="hljs-keyword">for</span> t, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(tokens, target):<br>        decoded =  tokenizer.tokenizer.index_special_tokens[t] \<br>            <span class="hljs-keyword">if</span> t <span class="hljs-keyword">in</span> tokenizer.tokenizer.index_special_tokens \<br>            <span class="hljs-keyword">else</span> tokenizer.decode([t])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">"%20s: %6d -&gt; %6d"</span> % (<span class="hljs-built_in">repr</span>(decoded), t, m))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">"&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; Sanity Check"</span>)<br><br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(tokens) == <span class="hljs-built_in">len</span>(target), <span class="hljs-string">f"length mismatch: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(tokens)}</span> vs <span class="hljs-subst">{<span class="hljs-built_in">len</span>(target)}</span>"</span><br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>
<h4 id="基本拼接方式">2.3.1 基本拼接方式</h4>
<p><img src="/images/batch.png" srcset="/img/loading.gif" lazyload></p>
<h4 id="多轮对话的拼接">2.3.2 多轮对话的拼接</h4>
<p><strong>ChatGLM 3 的方式</strong></p>
<p>因为 CausalLM
是一直从左往右预测的，我们可以直接在多轮对话中标识出多段输出。具体如下：</p>
<p>角色<strong>special
token</strong>用于标识分隔出多轮对话，同时也可以防范注入攻击</p>
<ul>
<li><code>&lt;|system|&gt;</code>
#系统提示词，指明模型可使用的工具等信息</li>
<li><code>&lt;|user|&gt;</code> #用户输入，用户的指令</li>
<li><code>&lt;|assistant|&gt;</code>
#模型回复，或模型思考要做的事情</li>
<li><code>&lt;|observation|&gt;</code> #工具调用、代码执行结果</li>
</ul>
<p>注意：<strong>这里&lt;|role|&gt;这种是一个
token</strong>，而不是一串文本，所以不能通过<code>tokenizer.encode('&lt;role&gt;')</code>来得到</p>
<p>角色后跟随的是 metadata，对于 function calling 来说，metadata
是调用的函数和相应参数；对其他角色的对话，metadata 为空</p>
<ul>
<li>多轮对话 finetune 时根据角色添加 loss_mask</li>
<li>在一遍计算中为多轮回复计算 loss</li>
</ul>
<hr>
<p><code>&lt;|system|&gt;</code>Answer the following questions as best
as you can.</p>
<p>You have access to the following tools:\n[…]</p>
<p><code>&lt;|user|&gt;</code> 北京的天气怎么样？</p>
<p><code>&lt;|assistant|&gt;</code><span style="background-color: yellow; color: black;">
我需要调用天气预报工具来获取北京的天气信息。</span></p>
<p><span style="background-color: yellow; color: brown;">&lt;|assistant|&gt;get_weather```python_call(location=“北京”)```</span></p>
<p><span style="background-color: yellow; color: black;">&lt;|observation|&gt;</span>
{“temperature_c”: 12, “description”: “haze”}</p>
<p><code>&lt;|assistant|&gt;</code><span style="background-color: yellow; color: black;">
根据天气工具的信息，北京的天气是：温度 12 摄氏度，有雾。</span></p>
<p><span style="background-color: yellow; color: black;">&lt;|user|&gt;</span>
这样的天气适合外出活动吗？</p>
<p><code>&lt;|assistant|&gt;</code><span style="background-color: yellow; color: black;">
北京现在有雾，气温较低，建议您考虑一下是否适合外出进行锻炼。</span></p>
<p><span style="background-color: yellow; color: black;">&lt;|user|&gt;</span></p>
<hr>
<p>高亮部分为需要计算 loss 的
token。<strong>注意&lt;|assistant|＞后的内容和角色 token 都需要算
loss。</strong></p>
<p>官方讲解：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1uC4y1J7yA">https://www.bilibili.com/video/BV1uC4y1J7yA</a></p>
<p><strong>多轮对话处理代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">format_conversation</span>(<span class="hljs-params">item, tokenizer, conversation_key: <span class="hljs-built_in">str</span>, tool_key: <span class="hljs-built_in">str</span></span>):<br>    conversations = deepcopy(item[conversation_key])<br><br>    <span class="hljs-comment"># Note: `loss_mask` here means whether *the prediction* of the token should take loss</span><br>    tokens, loss_masks = [tokenizer.get_command(<span class="hljs-string">"[gMASK]"</span>), tokenizer.get_command(<span class="hljs-string">"sop"</span>)], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_update</span>(<span class="hljs-params">_tokens: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], value: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span></span>):<br>        value = <span class="hljs-built_in">int</span>(value)<br>        tokens.extend(_tokens)<br>        loss_masks.extend([value] * <span class="hljs-built_in">len</span>(_tokens))<br><br>    <span class="hljs-comment"># insert system prompt for tools</span><br>    <span class="hljs-keyword">if</span> tool_key <span class="hljs-keyword">in</span> item:<br>        conversations.insert(<span class="hljs-number">0</span>, <br>            {<br>                <span class="hljs-string">"role"</span>: <span class="hljs-string">"system"</span>, <br>                <span class="hljs-string">"content"</span>: TOOL_DEFINITION_PREFIX + json.dumps(item[tool_key], indent=<span class="hljs-number">4</span>, ensure_ascii=<span class="hljs-literal">False</span>)<br>            }<br>        )<br>    <br>    <span class="hljs-keyword">for</span> idx, conv <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(conversations):<br>        loss = conv.get(<span class="hljs-string">"loss"</span>, <span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">if</span> conv[<span class="hljs-string">'role'</span>] <span class="hljs-keyword">in</span> {<span class="hljs-string">'system'</span>, <span class="hljs-string">'user'</span>}:<br>            loss = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> conv[<span class="hljs-string">'role'</span>] == <span class="hljs-string">'tool'</span>:<br>            <span class="hljs-comment"># function call python code</span><br>            value = FUNCTION_CALL_PREFIX + format_function_call(FUNCTION_CALL_NAME, conv[<span class="hljs-string">"parameters"</span>]) + FUNCTION_CALL_POSTFIX<br>            text = tokenizer.build_single_message(<span class="hljs-string">"assistant"</span>, conv[<span class="hljs-string">"name"</span>], value)<br>            _update(text, loss)<br><br>            <span class="hljs-comment"># function call result</span><br>            value = conv.get(<span class="hljs-string">'observation'</span>, <span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(value, <span class="hljs-built_in">str</span>):<br>                value = json.dumps(value, ensure_ascii=<span class="hljs-literal">False</span>)<br>            text = tokenizer.build_single_message(<span class="hljs-string">"observation"</span>, <span class="hljs-string">""</span>, value)<br>            _update(text, <span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">else</span>:<br>            text = tokenizer.build_single_message(conv[<span class="hljs-string">'role'</span>], <span class="hljs-string">""</span>, conv[<span class="hljs-string">"content"</span>])<br>            _update(text, loss)<br><br>    _update([tokenizer.eos_token_id], <span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(tokens) == <span class="hljs-built_in">len</span>(loss_masks), <span class="hljs-string">f"length mismatch: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(tokens)}</span> vs <span class="hljs-subst">{<span class="hljs-built_in">len</span>(loss_masks)}</span>"</span><br>    <span class="hljs-keyword">return</span> tokens, loss_masks<br></code></pre></td></tr></table></figure>
<h4 id="chatglm-3-的数据加载">2.3.3 ChatGLM 3 的数据加载</h4>
<p>实际应用中，只需要将上述数据，与 ChatGLM 3
的标准数据格式对齐，就可调用其原生的数据加载器，自动完成数据拼接</p>
<p>ChatGLM 3 官方近期重构了代码，数据加载部分在：<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM3/blob/main/finetune_demo/finetune_hf.py">https://github.com/THUDM/ChatGLM3/blob/main/finetune_demo/finetune_hf.py</a></p>
<p>但这版重构后的代码未实现 tool 的拼接部分。带有 tool
拼接的早期版本，参考：<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM3/blob/4568c635e686e8e2053568d041f36c884cab328a/finetune_demo/preprocess_utils.py">https://github.com/THUDM/ChatGLM3/blob/4568c635e686e8e2053568d041f36c884cab328a/finetune_demo/preprocess_utils.py</a></p>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-05eef982" role="button" aria-expanded="false" aria-controls="collapse-05eef982">
        <div class="fold-arrow">▶</div>格式化后的数据样例
      </div>
      <div class="fold-collapse collapse" id="collapse-05eef982">
        <div class="fold-content">
          <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">{</span><br>  <span class="hljs-attr">"tools"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>    <span class="hljs-string">"search_hotels: 根据筛选条件查询酒店的函数\nparameters: {\"name\":\"酒店名称\",\"price_range_lower\":\"价格下限\",\"price_range_upper\":\"价格上限\",\"rating_range_lower\":\"评分下限\",\"rating_range_upper\":\"评分上限\",\"facilities\": \"酒店提供的设施\"}\noutput: 酒店信息dict组成的list"</span><br>  <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">"conversations"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">{</span><br>      <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"user"</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"请帮我找一家最低价格是300-400元，提供无烟房的经济型酒店。"</span><br>    <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">{</span><br>      <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"assistant"</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"我需要使用search_hotels工具来查询酒店"</span><br>    <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">{</span><br>      <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"tool"</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"search_hotels"</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">"parameters"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><br>        <span class="hljs-attr">"facilities"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">"无烟房"</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"price_range_lower"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">300</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"price_range_upper"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">400</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"经济型"</span><br>      <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">"observation"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">{</span><br>          <span class="hljs-attr">"name"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"飘HOME连锁酒店(北京王府井店)"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-attr">"type"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"经济型"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-attr">"address"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"北京东城区东安门大街43号"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-attr">"subway"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"灯市口地铁站A口"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-attr">"phone"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"010-57305888"</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-attr">"facilities"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-string">"酒店各处提供wifi"</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">"宽带上网"</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">"吹风机"</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">"24小时热水"</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">"暖气"</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">"无烟房"</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">"早餐服务"</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">"行李寄存"</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-string">"叫醒服务"</span><br>          <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-attr">"price"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">303.0</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-attr">"rating"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4.3</span><span class="hljs-punctuation">,</span><br>          <span class="hljs-attr">"hotel_id"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">152</span><br>        <span class="hljs-punctuation">}</span><br>      <span class="hljs-punctuation">]</span><br>    <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">{</span><br>      <span class="hljs-attr">"role"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"assistant"</span><span class="hljs-punctuation">,</span><br>      <span class="hljs-attr">"content"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"推荐您去飘HOME连锁酒店(北京王府井店)。"</span><br>    <span class="hljs-punctuation">}</span><br>  <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">}</span><br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>
<p><strong>数据加载代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiTurnDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data: <span class="hljs-type">List</span>[<span class="hljs-built_in">dict</span>], tokenizer: PreTrainedTokenizer, max_seq_length: <span class="hljs-built_in">int</span></span>):<br>        <span class="hljs-built_in">super</span>(MultiTurnDataset, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.tokenizer = tokenizer<br>        <span class="hljs-variable language_">self</span>.max_seq_length = max_seq_length<br>        <span class="hljs-variable language_">self</span>.data = data<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, i</span>) -&gt; <span class="hljs-built_in">dict</span>:<br>        data_item = <span class="hljs-variable language_">self</span>.data[i]<br>        tokens, loss_masks = format_conversation(data_item, <span class="hljs-variable language_">self</span>.tokenizer, CONVERSATOIN_KEY, TOOL_DESC_KEY)<br><br>        <span class="hljs-comment"># labels are used inside the model</span><br>        target_based_loss_mask = [<span class="hljs-literal">False</span>] + loss_masks[:-<span class="hljs-number">1</span>]<br>        labels = [(t <span class="hljs-keyword">if</span> m <span class="hljs-keyword">else</span> -<span class="hljs-number">100</span>) <span class="hljs-keyword">for</span> t, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(tokens, target_based_loss_mask)]<br><br>        tokens = tokens[:<span class="hljs-variable language_">self</span>.max_seq_length]<br>        labels = labels[:<span class="hljs-variable language_">self</span>.max_seq_length]<br>        tokens += [<span class="hljs-variable language_">self</span>.tokenizer.pad_token_id] * (<span class="hljs-variable language_">self</span>.max_seq_length - <span class="hljs-built_in">len</span>(tokens))<br>        labels += [-<span class="hljs-number">100</span>] * (<span class="hljs-variable language_">self</span>.max_seq_length - <span class="hljs-built_in">len</span>(labels))<br><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(tokens) == <span class="hljs-built_in">len</span>(labels), <span class="hljs-string">f"length mismatch: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(tokens)}</span> vs <span class="hljs-subst">{<span class="hljs-built_in">len</span>(labels)}</span>"</span><br><br>        <span class="hljs-keyword">return</span> {<br>            <span class="hljs-string">"input_ids"</span>: tokens,<br>            <span class="hljs-string">"labels"</span>: labels<br>        }{% endfold %}<br> <br></code></pre></td></tr></table></figure>
<h3 id="chatglm-3-prefix-tunning训练代码">2.4 ChatGLM 3
Prefix-Tunning训练代码</h3>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-16a56b89" role="button" aria-expanded="false" aria-controls="collapse-16a56b89">
        <div class="fold-arrow">▶</div>模型定义和训练过程中的命令行参数
      </div>
      <div class="fold-collapse collapse" id="collapse-16a56b89">
        <div class="fold-content">
          <p><strong>模型相关参数</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">"""</span><br><span class="hljs-string">文件中定义了模型定义和训练过程中的命令行参数</span><br><span class="hljs-string">"""</span><br><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass, field<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span><br><br><br><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ModelArguments</span>:<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.</span><br><span class="hljs-string">    """</span><br><br>    model_name_or_path: <span class="hljs-built_in">str</span> = field(<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Path to pretrained model or model identifier from huggingface.co/models"</span>}<br>    )<br>    checkpoint_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Path to pt2 or lora finetuned checkpoint dir"</span>}<br>    )<br>    ptuning_checkpoint: <span class="hljs-built_in">str</span> = field(<br>        default=<span class="hljs-literal">None</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Path to p-tuning v2 checkpoints"</span>}<br>    )<br>    config_name: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Pretrained config name or path if not the same as model_name"</span>}<br>    )<br>    tokenizer_name: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Pretrained tokenizer name or path if not the same as model_name"</span>}<br>    )<br>    cache_dir: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Where to store the pretrained models downloaded from huggingface.co"</span>},<br>    )<br>    use_fast_tokenizer: <span class="hljs-built_in">bool</span> = field(<br>        default=<span class="hljs-literal">True</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."</span>},<br>    )<br>    model_revision: <span class="hljs-built_in">str</span> = field(<br>        default=<span class="hljs-string">"main"</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The specific model version to use (can be a branch name, tag name or commit id)."</span>},<br>    )<br>    use_auth_token: <span class="hljs-built_in">bool</span> = field(<br>        default=<span class="hljs-literal">False</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"Will use the token generated when running `huggingface-cli login` (necessary to use this script "</span><br>                <span class="hljs-string">"with private models)."</span><br>            )<br>        },<br>    )<br>    resize_position_embeddings: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"Whether to automatically resize the position embeddings if `max_source_length` exceeds "</span><br>                <span class="hljs-string">"the model's position embeddings."</span><br>            )<br>        },<br>    )<br>    quantization_bit: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-literal">None</span><br>    )<br>    pre_seq_len: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-literal">None</span><br>    )<br>    prefix_projection: <span class="hljs-built_in">bool</span> = field(<br>        default=<span class="hljs-literal">False</span><br>    )<br></code></pre></td></tr></table></figure></p><p><strong>数据相关参数</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataTrainingArguments</span>:<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    Arguments pertaining to what data we are going to input our model for training and eval.</span><br><span class="hljs-string">    """</span><br>    train_file: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The input training data file (a jsonlines or csv file)."</span>}<br>    )<br>    validation_file: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The input validation data file (a jsonlines or csv file)."</span>}<br>    )<br>    test_file: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The input test data file (a jsonlines or csv file)."</span>}<br>    )<br><br>    max_seq_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-number">2048</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"The maximum total input sequence length after tokenization. Sequences longer "</span><br>                <span class="hljs-string">"than this will be truncated."</span><br>            )<br>        },<br>    )<br><br>    max_source_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-number">1024</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"The maximum total input sequence length after tokenization. Sequences longer "</span><br>                <span class="hljs-string">"than this will be truncated, sequences shorter will be padded."</span><br>            )<br>        },<br>    )<br>    max_target_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-number">128</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"The maximum total sequence length for target text after tokenization. Sequences longer "</span><br>                <span class="hljs-string">"than this will be truncated, sequences shorter will be padded."</span><br>            )<br>        },<br>    )<br><br>    overwrite_cache: <span class="hljs-built_in">bool</span> = field(<br>        default=<span class="hljs-literal">False</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Overwrite the cached training and evaluation sets"</span>}<br>    )<br><br>    preprocessing_num_workers: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The number of processes to use for the preprocessing."</span>},<br>    )<br><br>    max_seq_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-number">1024</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"The maximum total input sequence length after tokenization. Sequences longer "</span><br>                <span class="hljs-string">"than this will be truncated, sequences shorter will be padded."</span><br>            )<br>        },<br>    )<br><br>    pad_to_max_length: <span class="hljs-built_in">bool</span> = field(<br>        default=<span class="hljs-literal">False</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"Whether to pad all samples to model maximum sentence length. "</span><br>                <span class="hljs-string">"If False, will pad the samples dynamically when batching to the maximum length in the batch. More "</span><br>                <span class="hljs-string">"efficient on GPU but very bad for TPU."</span><br>            )<br>        },<br>    )<br><br>    max_train_samples: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"For debugging purposes or quicker training, truncate the number of training examples to this "</span><br>                <span class="hljs-string">"value if set."</span><br>            )<br>        },<br>    )<br></code></pre></td></tr></table></figure></p><p><strong>微调相关参数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PeftArguments</span>:<br>    lora_rank: <span class="hljs-built_in">int</span> = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"LoRA rank number"</span>}<br>    )<br>    lora_alpha: <span class="hljs-built_in">int</span> = field(<br>        default=<span class="hljs-number">32</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"LoRA alpha weight"</span>}<br>    )<br>    lora_dropout: <span class="hljs-built_in">float</span> = field(<br>        default=<span class="hljs-number">0.1</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"LoRA dropout probability"</span>}<br>    )<br>    lora_checkpoint: <span class="hljs-built_in">str</span> = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Path to LoRA checkpoints"</span>}<br>    )<br><br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-8bbf6858" role="button" aria-expanded="false" aria-controls="collapse-8bbf6858">
        <div class="fold-arrow">▶</div>PrefixTrainer定义
      </div>
      <div class="fold-collapse collapse" id="collapse-8bbf6858">
        <div class="fold-content">
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">"""</span><br><span class="hljs-string">The Trainer class, to easily train a 🤗 Transformers from scratch or finetune it on a new task.</span><br><span class="hljs-string">"""</span><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers.modeling_utils <span class="hljs-keyword">import</span> PreTrainedModel, unwrap_model<br><span class="hljs-keyword">from</span> transformers.utils <span class="hljs-keyword">import</span> logging<br><br>logger = logging.get_logger(__name__)<br><br>WEIGHTS_NAME = <span class="hljs-string">"pytorch_model.bin"</span><br>TRAINING_ARGS_NAME = <span class="hljs-string">"training_args.bin"</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PrefixTrainer</span>(<span class="hljs-title class_ inherited__">Trainer</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *args, save_changed=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-variable language_">self</span>.save_changed = save_changed<br>        <span class="hljs-built_in">super</span>().__init__(*args, **kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_save</span>(<span class="hljs-params">self, output_dir: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>, state_dict=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># If we are executing this function, we are the process zero, so we don't check for that.</span><br>        output_dir = output_dir <span class="hljs-keyword">if</span> output_dir <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-variable language_">self</span>.args.output_dir<br>        os.makedirs(output_dir, exist_ok=<span class="hljs-literal">True</span>)<br>        logger.info(<span class="hljs-string">f"Saving model checkpoint to <span class="hljs-subst">{output_dir}</span>"</span>)<br>        <span class="hljs-comment"># Save a trained model and configuration using `save_pretrained()`.</span><br>        <span class="hljs-comment"># They can then be reloaded using `from_pretrained()`</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(<span class="hljs-variable language_">self</span>.model, PreTrainedModel):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(unwrap_model(<span class="hljs-variable language_">self</span>.model), PreTrainedModel):<br>                <span class="hljs-keyword">if</span> state_dict <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    state_dict = <span class="hljs-variable language_">self</span>.model.state_dict()<br>                unwrap_model(<span class="hljs-variable language_">self</span>.model).save_pretrained(output_dir, safe_serialization=<span class="hljs-literal">False</span>, state_dict=state_dict)<br>            <span class="hljs-keyword">else</span>:<br>                logger.info(<span class="hljs-string">"Trainer.model is not a `PreTrainedModel`, only saving its state dict."</span>)<br>                <span class="hljs-keyword">if</span> state_dict <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    state_dict = <span class="hljs-variable language_">self</span>.model.state_dict()<br>                torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.save_changed:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">"Saving PrefixEncoder"</span>)<br>                state_dict = <span class="hljs-variable language_">self</span>.model.state_dict()<br>                filtered_state_dict = {}<br>                <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.model.named_parameters():<br>                    <span class="hljs-keyword">if</span> v.requires_grad:<br>                        filtered_state_dict[k] = state_dict[k]<br>                <span class="hljs-variable language_">self</span>.model.save_pretrained(output_dir, safe_serialization=<span class="hljs-literal">False</span>, state_dict=filtered_state_dict)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">"Saving the whole model"</span>)<br>                <span class="hljs-variable language_">self</span>.model.save_pretrained(output_dir, safe_serialization=<span class="hljs-literal">False</span>, state_dict=state_dict)<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.tokenizer <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-variable language_">self</span>.tokenizer.save_pretrained(output_dir, safe_serialization=<span class="hljs-literal">False</span>)<br><br>        <span class="hljs-comment"># Good practice: save your training arguments together with the trained model</span><br>        torch.save(<span class="hljs-variable language_">self</span>.args, os.path.join(output_dir, TRAINING_ARGS_NAME))<br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-8e159bd8" role="button" aria-expanded="false" aria-controls="collapse-8e159bd8">
        <div class="fold-arrow">▶</div>日志记录
      </div>
      <div class="fold-collapse collapse" id="collapse-8e159bd8">
        <div class="fold-content">
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> transformers<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (<br>    AutoConfig,<br>    AutoModel,<br>    AutoTokenizer,<br>    DataCollatorForSeq2Seq,<br>    HfArgumentParser,<br>    TrainingArguments,<br>    set_seed,<br>)<br><span class="hljs-keyword">from</span> trainer <span class="hljs-keyword">import</span> PrefixTrainer<br><span class="hljs-keyword">from</span> arguments <span class="hljs-keyword">import</span> ModelArguments, DataTrainingArguments<br><span class="hljs-keyword">from</span> data_preprocess <span class="hljs-keyword">import</span> sanity_check, MultiTurnDataset<br><br><span class="hljs-comment"># 初始化日志记录</span><br>logger = logging.getLogger(__name__)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_logger</span>(<span class="hljs-params">training_args</span>):<br>    logging.basicConfig(<br>        <span class="hljs-built_in">format</span>=<span class="hljs-string">"%(asctime)s - %(levelname)s - %(name)s - %(message)s"</span>,<br>        datefmt=<span class="hljs-string">"%m/%d/%Y %H:%M:%S"</span>,<br>        handlers=[logging.StreamHandler(sys.stdout)],<br>    )<br>    <span class="hljs-comment"># 配置huggingface的日志记录</span><br>    <span class="hljs-keyword">if</span> training_args.should_log:<br>        transformers.utils.logging.set_verbosity_info()<br>    log_level = training_args.get_process_log_level()<br>    logger.setLevel(log_level)<br>    transformers.utils.logging.set_verbosity(log_level)<br>    transformers.utils.logging.enable_default_handler()<br>    transformers.utils.logging.enable_explicit_format()<br><br>    logger.warning(<br>        <span class="hljs-string">f"Process rank: <span class="hljs-subst">{training_args.local_rank}</span>, device: <span class="hljs-subst">{training_args.device}</span>, n_gpu: <span class="hljs-subst">{training_args.n_gpu}</span>"</span><br>        + <span class="hljs-string">f"distributed training: <span class="hljs-subst">{<span class="hljs-built_in">bool</span>(training_args.local_rank != -<span class="hljs-number">1</span>)}</span>, 16-bits training: <span class="hljs-subst">{training_args.fp16}</span>"</span><br>    )<br>    logger.info(<span class="hljs-string">f"Training/evaluation parameters <span class="hljs-subst">{training_args}</span>"</span>)<br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>
<p><strong>加载模型</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">model_args</span>):<br>    <span class="hljs-comment"># 加载预训练的chatglm3-6b的model config</span><br>    config = AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=<span class="hljs-literal">True</span>)<br>    config.pre_seq_len = model_args.pre_seq_len<br>    config.prefix_projection = model_args.prefix_projection<br>    <span class="hljs-comment"># 加载预训练的chatglm3-6b的tokenizer</span><br>    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># 判断是否加载pt2的checkpoint来继续训练</span><br>    <span class="hljs-keyword">if</span> model_args.ptuning_checkpoint <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        model = AutoModel.from_pretrained(model_args.model_name_or_path, config=config, trust_remote_code=<span class="hljs-literal">True</span>)<br>        prefix_state_dict = torch.load(os.path.join(model_args.ptuning_checkpoint, <span class="hljs-string">"pytorch_model.bin"</span>))<br>        new_prefix_state_dict = {}<br>        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> prefix_state_dict.items():<br>            <span class="hljs-keyword">if</span> k.startswith(<span class="hljs-string">"transformer.prefix_encoder."</span>):<br>                new_prefix_state_dict[k[<span class="hljs-built_in">len</span>(<span class="hljs-string">"transformer.prefix_encoder."</span>):]] = v<br>        model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)<br>    <span class="hljs-keyword">else</span>: <span class="hljs-comment"># 不加载pt2 checkpoint则直接加载model</span><br>        model = AutoModel.from_pretrained(model_args.model_name_or_path, config=config, trust_remote_code=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># 如果有设置quantization则以int数值加载不参与更新的参数，用以节省显存</span><br>    <span class="hljs-keyword">if</span> model_args.quantization_bit <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Quantized to <span class="hljs-subst">{model_args.quantization_bit}</span> bit"</span>)<br>        model = model.quantize(model_args.quantization_bit)<br>    <span class="hljs-comment"># pt2训练，为要训练的prefix_encoder参数使用更高数值精度的float32</span><br>    <span class="hljs-keyword">if</span> model_args.pre_seq_len <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        model = model.half()<br>        model.transformer.prefix_encoder.<span class="hljs-built_in">float</span>()<br>    <span class="hljs-comment"># 全量参数finetune训练，需要很高的显存配置</span><br>    <span class="hljs-keyword">else</span>:<br>        model = model.<span class="hljs-built_in">float</span>()<br>    <br>    <span class="hljs-keyword">return</span> tokenizer, model<br></code></pre></td></tr></table></figure>
<p><strong>Prefix-Tuning微调主流程</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 解析传入的命令行参数</span><br>    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))<br>    model_args, data_args, training_args = parser.parse_args_into_dataclasses()<br>    <span class="hljs-comment"># 初始化工作</span><br>    setup_logger(training_args)<br>    set_seed(training_args.seed)<br>    tokenizer, model = load_model(model_args)<br>    <span class="hljs-comment"># 准备训练数据集并处理成所需格式</span><br>    <span class="hljs-keyword">if</span> training_args.do_train:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(data_args.train_file, <span class="hljs-string">"r"</span>, encoding=<span class="hljs-string">"utf-8"</span>) <span class="hljs-keyword">as</span> f:<br>            train_data = [json.loads(line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f]<br><br>        train_dataset = MultiTurnDataset(<br>            train_data,<br>            tokenizer,<br>            data_args.max_seq_length,<br>        )<br><br>        <span class="hljs-comment">#if training_args.local_rank &lt; 1:</span><br>        <span class="hljs-comment">#    sanity_check(train_dataset[0]['input_ids'], train_dataset[0]['labels'], tokenizer)</span><br>    <span class="hljs-keyword">if</span> training_args.do_eval:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(data_args.validation_file, <span class="hljs-string">"r"</span>, encoding=<span class="hljs-string">"utf-8"</span>) <span class="hljs-keyword">as</span> f:<br>            eval_data = [json.loads(line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f]<br><br>        eval_dataset = MultiTurnDataset(<br>            eval_data,<br>            tokenizer,<br>            data_args.max_seq_length,<br>        )<br>    <span class="hljs-comment"># 将数据集中样本批处理成张量</span><br>    data_collator = DataCollatorForSeq2Seq(<br>        tokenizer,<br>        model=model,<br>        label_pad_token_id=-<span class="hljs-number">100</span>,<br>        pad_to_multiple_of=<span class="hljs-literal">None</span>,<br>        padding=<span class="hljs-literal">False</span><br>    )<br>    <span class="hljs-comment"># 配置trainer，相比base trainer重写了保存参数的功能</span><br>    trainer = PrefixTrainer(<br>        model=model,<br>        args=training_args,<br>        train_dataset=train_dataset <span class="hljs-keyword">if</span> training_args.do_train <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        eval_dataset=eval_dataset <span class="hljs-keyword">if</span> training_args.do_eval <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        tokenizer=tokenizer,<br>        data_collator=data_collator,<br>        save_changed=model_args.pre_seq_len <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>    )<br>    <span class="hljs-comment"># 开始训练</span><br>    <span class="hljs-keyword">if</span> training_args.do_train:<br>        checkpoint = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> training_args.resume_from_checkpoint <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            checkpoint = training_args.resume_from_checkpoint<br>        model.gradient_checkpointing_enable()<br>        model.enable_input_require_grads()<br>        trainer.train(resume_from_checkpoint=checkpoint)<br>        trainer.save_model()<br>        trainer.save_state()<br>    <span class="hljs-keyword">if</span> training_args.do_eval:<br>        trainer.evaluate()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:<br>    main()<br></code></pre></td></tr></table></figure>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-c21ca059" role="button" aria-expanded="false" aria-controls="collapse-c21ca059">
        <div class="fold-arrow">▶</div>训练脚本
      </div>
      <div class="fold-collapse collapse" id="collapse-c21ca059">
        <div class="fold-content">
          <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /usr/bin/env bash</span><br><br><span class="hljs-built_in">set</span> -ex<br><br>LR=2e-2<br>PRE_SEQ_LEN=256<br>MAX_SEQ_LEN=3072<br><br>DATESTR=`<span class="hljs-built_in">date</span> +%Y%m%d-%H%M%S`<br>RUN_NAME=hotel_pt2<br>OUTPUT_DIR=output/<span class="hljs-variable">${RUN_NAME}</span>-<span class="hljs-variable">${DATESTR}</span><br><span class="hljs-built_in">mkdir</span> -p <span class="hljs-variable">$OUTPUT_DIR</span><br><br>BASE_MODEL_PATH=/pathto/chatglm3-6b<br><br>CUDA_VISIBLE_DEVICES=0 python Prefix_Tunning.py \<br>    --do_train \<br>    --do_eval \<br>    --train_file ../data/train.chatglm3.jsonl \<br>    --validation_file ../data/dev.chatglm3.jsonl \<br>    --max_seq_length <span class="hljs-variable">$MAX_SEQ_LEN</span> \<br>    --preprocessing_num_workers 1 \<br>    --model_name_or_path <span class="hljs-variable">$BASE_MODEL_PATH</span> \<br>    --output_dir <span class="hljs-variable">$OUTPUT_DIR</span> \<br>    --per_device_train_batch_size 2 \<br>    --gradient_accumulation_steps 2 \<br>    --per_device_eval_batch_size 2 \<br>    --evaluation_strategy steps \<br>    --eval_steps 300 \<br>    --num_train_epochs 6 \<br>    --logging_steps 300 \<br>    --logging_dir <span class="hljs-variable">$OUTPUT_DIR</span>/logs \<br>    --save_steps 300 \<br>    --learning_rate <span class="hljs-variable">$LR</span> \<br>    --quantization_bit 4 \<br>    --pre_seq_len <span class="hljs-variable">$PRE_SEQ_LEN</span> 2&gt;&amp;1 | <span class="hljs-built_in">tee</span> <span class="hljs-variable">${OUTPUT_DIR}</span>/train.log<br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div></p>
<h2 id="三llama-3-lora-训练">三、Llama 3 LoRA 训练</h2>
<p>基于Llama3微调一个同时具有 NLU 和问答能力对话机器人</p>
<h3 id="llama-3-中实现类似-function-calling-的效果">3.1 Llama 3
中实现类似 Function Calling 的效果</h3>
<p>类似 ChatGLM 3 的实现方式</p>
<ol type="1">
<li>自定义 user、assistant、search、return 四个角色
<ul>
<li>因为只有一个 function，我们直接把 function 标识成 search</li>
</ul></li>
<li>每轮 assistant 和 search 前缀也由模型自动生成，我们以此判断是
function 还是文本回复</li>
<li>类似 ChatGLM 3，我们以预留的特殊
token，来标识每个轮次的角色和轮次结束
<ul>
<li>例如：<code>&lt;|start_header_id|&gt;</code><em>角色</em><code>&lt;|end_header_id|&gt;</code><em>内容
… …</em><code>&lt;|eot_id|&gt;</code></li>
<li>其中
<code>&lt;|start_header_id|&gt;</code>，<code>&lt;|end_header_id|&gt;</code>，<code>&lt;|eot_id|&gt;</code>
是 Llama 3 预留的特殊 token</li>
</ul></li>
</ol>
<h4 id="function-call-的样例">3.1.1 Function Call 的样例</h4>
<p><strong>输入</strong></p>
<p><code>&lt;|start_header_id|&gt;</code>user<code>&lt;|end_header_id|&gt;</code>你好，请帮我推荐一个提供无烟房的舒适型酒店可以吗？<code>&lt;|eot_id|&gt;</code></p>
<p><strong>输出</strong></p>
<p><code>&lt;|start_header_id|&gt;</code>search<code>&lt;|end_header_id|&gt;</code>{"facilities":
["无烟房"], "type": "舒适型"}}<code>&lt;|eot_id|&gt;</code></p>
<h4 id="文本回复的样例">3.1.2 文本回复的样例</h4>
<p><strong>输入</strong></p>
<p><code>&lt;|start_header_id|&gt;</code>user<code>&lt;|end_header_id|&gt;</code>你好，请帮我推荐一个提供无烟房的舒适型酒店可以吗？<code>&lt;|eot_id|&gt;</code></p>
<p><code>&lt;|start_header_id|&gt;</code>search<code>&lt;|end_header_id|&gt;</code>{"facilities":
["无烟房"], "type": "舒适型"}}<code>&lt;|eot_id|&gt;</code></p>
<p><code>&lt;|start_header_id|&gt;</code>return<code>&lt;|end_header_id|&gt;</code>[{"name":
"北京红驿栈酒店", "type": "舒适型", "address":
"北京朝阳区东直门外春秀路太平庄 10 号(主副楼在一幢建筑里)", "subway":
"东直门地铁站 E 口", "phone": "010-64171066", "facilities":
["公共区域和部分房间提供 wifi", "宽带上网", "国际长途电话", "吹风机",
"24 小时热水", "暖气", "无烟房", "早餐服务", "接待外宾", "行李寄存",
"叫醒服务"], "price": 344.0, "rating": 4.7, "hotel_id": 51}, {"name":
"维也纳国际酒店(北京广安门店)", "type": "舒适型", "address":
"北京西城区白广路 7 号", "subway": "广安门内地铁站 C 口", "phone":
"010-83539988", "facilities": ["酒店各处提供 wifi", "宽带上网",
"吹风机", "24 小时热水", "中式餐厅", "会议室", "无烟房", "商务中心",
"早餐服务", "洗衣服务", "行李寄存", "叫醒服务"], "price": 553.0,
"rating": 4.7, "hotel_id": 56}]}]<code>&lt;|eot_id|&gt;</code></p>
<p><strong>输出</strong></p>
<p><code>&lt;|start_header_id|&gt;</code>assistant<code>&lt;|end_header_id|&gt;</code>没问题，推荐你去北京红驿栈酒店和维也纳国际酒店(北京广安门店)，都挺好的。<code>&lt;|eot_id|&gt;</code></p>
<h3 id="lora-代码">3.2 LoRA 代码</h3>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-0fdefa4a" role="button" aria-expanded="false" aria-controls="collapse-0fdefa4a">
        <div class="fold-arrow">▶</div>命令行参数配置
      </div>
      <div class="fold-collapse collapse" id="collapse-0fdefa4a">
        <div class="fold-content">
          <p><strong>模型配置</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass, field<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span><br><br><br><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ModelArguments</span>:<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.</span><br><span class="hljs-string">    """</span><br>    model_name_or_path: <span class="hljs-built_in">str</span> = field(<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Path to pretrained model or model identifier from huggingface.co/models"</span>}<br>    )<br></code></pre></td></tr></table></figure> <strong>LoRA配置</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PeftArguments</span>:<br>    lora_rank: <span class="hljs-built_in">int</span> = field(<br>        default=<span class="hljs-number">8</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"LoRA rank number"</span>}<br>    )<br>    lora_alpha: <span class="hljs-built_in">int</span> = field(<br>        default=<span class="hljs-number">32</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"LoRA alpha weight"</span>}<br>    )<br>    lora_dropout: <span class="hljs-built_in">float</span> = field(<br>        default=<span class="hljs-number">0.1</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"LoRA dropout probability"</span>}<br>    )<br>    lora_checkpoint: <span class="hljs-built_in">str</span> = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Path to LoRA checkpoints"</span>}<br>    )<br></code></pre></td></tr></table></figure></p><p><strong>数据处理配置</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataTrainingArguments</span>:<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    Arguments pertaining to what data we are going to input our model for training and eval.</span><br><span class="hljs-string">    """</span><br>    prompt_column: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The name of the column in the datasets containing the full texts (for summarization)."</span>},<br>    )<br>    response_column: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The name of the column in the datasets containing the summaries (for summarization)."</span>},<br>    )<br>    history_column: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The name of the column in the datasets containing the history of chat."</span>},<br>    )<br>    train_file: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The input training data file (a jsonlines or csv file)."</span>}<br>    )<br>    validation_file: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file)."</span><br>            )<br>        },<br>    )<br>    test_file: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: <span class="hljs-string">"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file)."</span><br>        },<br>    )<br>    overwrite_cache: <span class="hljs-built_in">bool</span> = field(<br>        default=<span class="hljs-literal">False</span>, metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"Overwrite the cached training and evaluation sets"</span>}<br>    )<br>    preprocessing_num_workers: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-literal">None</span>,<br>        metadata={<span class="hljs-string">"help"</span>: <span class="hljs-string">"The number of processes to use for the preprocessing."</span>},<br>    )<br>    max_source_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-number">1024</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"The maximum total input sequence length after tokenization. Sequences longer "</span><br>                <span class="hljs-string">"than this will be truncated, sequences shorter will be padded."</span><br>            )<br>        },<br>    )<br>    max_target_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = field(<br>        default=<span class="hljs-number">256</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: (<br>                <span class="hljs-string">"The maximum total sequence length for target text after tokenization. Sequences longer "</span><br>                <span class="hljs-string">"than this will be truncated, sequences shorter will be padded."</span><br>            )<br>        },<br>    )<br>    ignore_pad_token_for_loss: <span class="hljs-built_in">bool</span> = field(<br>        default=<span class="hljs-literal">True</span>,<br>        metadata={<br>            <span class="hljs-string">"help"</span>: <span class="hljs-string">"Whether to ignore the tokens corresponding to padded labels in the loss computation or not."</span><br>        },<br>    )<br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-05b78603" role="button" aria-expanded="false" aria-controls="collapse-05b78603">
        <div class="fold-arrow">▶</div>数据处理代码
      </div>
      <div class="fold-collapse collapse" id="collapse-05b78603">
        <div class="fold-content">
          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InputOutputDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data, tokenizer, args</span>):<br>        <span class="hljs-built_in">super</span>(InputOutputDataset, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.data = data<br>        <span class="hljs-variable language_">self</span>.tokenizer = tokenizer<br>        <span class="hljs-variable language_">self</span>.prompt_column = args.prompt_column<br>        <span class="hljs-variable language_">self</span>.response_column = args.response_column<br>        <span class="hljs-variable language_">self</span>.max_source_length = args.max_source_length<br>        <span class="hljs-variable language_">self</span>.max_target_length = args.max_target_length<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, i</span>):<br>        item = <span class="hljs-variable language_">self</span>.data[i]<br>        <span class="hljs-comment"># add_special_tokens 不在开头加 special_tokens</span><br>        context = <span class="hljs-variable language_">self</span>.tokenizer(<br>            build_prompt(item[<span class="hljs-variable language_">self</span>.prompt_column]), <br>            max_length=<span class="hljs-variable language_">self</span>.max_source_length, <br>            add_special_tokens=<span class="hljs-literal">False</span>)<br>        response = <span class="hljs-variable language_">self</span>.tokenizer(<br>            build_response(item[<span class="hljs-variable language_">self</span>.response_column]), <br>            max_length=<span class="hljs-variable language_">self</span>.max_target_length, <br>            add_special_tokens=<span class="hljs-literal">False</span>)<br>        input_ids = context[<span class="hljs-string">"input_ids"</span>] + response[<span class="hljs-string">"input_ids"</span>]<br>        attention_mask = context[<span class="hljs-string">"attention_mask"</span>] + response[<span class="hljs-string">"attention_mask"</span>]<br>        labels = [-<span class="hljs-number">100</span>] * <span class="hljs-built_in">len</span>(context[<span class="hljs-string">"input_ids"</span>]) + response[<span class="hljs-string">"input_ids"</span>]<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(input_ids) == <span class="hljs-built_in">len</span>(labels), <span class="hljs-string">f"length mismatch: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(input_ids)}</span> vs <span class="hljs-subst">{<span class="hljs-built_in">len</span>(labels)}</span>"</span><br>        <span class="hljs-keyword">return</span> {<br>            <span class="hljs-string">"input_ids"</span>: input_ids,<br>            <span class="hljs-string">"attention_mask"</span>: attention_mask,<br>            <span class="hljs-string">"labels"</span>: labels<br>        }<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_prompt</span>(<span class="hljs-params">context</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(context,<span class="hljs-built_in">str</span>):<br>        context = json.loads(context)<br>    prompt = <span class="hljs-string">''</span><br>    <span class="hljs-keyword">for</span> turn <span class="hljs-keyword">in</span> context:<br>        <span class="hljs-keyword">if</span> turn[<span class="hljs-string">"role"</span>] <span class="hljs-keyword">in</span> [<span class="hljs-string">"user"</span>,<span class="hljs-string">"assistant"</span>]:<br>            prompt += <span class="hljs-string">f'&lt;|start_header_id|&gt;<span class="hljs-subst">{turn[<span class="hljs-string">"role"</span>]}</span>&lt;|end_header_id|&gt;\n<span class="hljs-subst">{turn[<span class="hljs-string">"content"</span>]}</span>&lt;|eot_id|&gt;\n'</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> turn[<span class="hljs-string">"role"</span>] == <span class="hljs-string">"search"</span>:<br>                obj = turn[<span class="hljs-string">"arguments"</span>]<br>                filtered_obj = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> obj.items() <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>}<br>                prompt += <span class="hljs-string">'&lt;|start_header_id|&gt;search&lt;|end_header_id|&gt;\n'</span><br>                prompt += json.dumps(filtered_obj,indent=<span class="hljs-number">4</span>,ensure_ascii=<span class="hljs-literal">False</span>)<br>            <span class="hljs-keyword">else</span>:<br>                obj = turn[<span class="hljs-string">"records"</span>]<br>                prompt += <span class="hljs-string">'&lt;|start_header_id|&gt;return&lt;|end_header_id|&gt;\n'</span><br>                prompt += json.dumps(obj,indent=<span class="hljs-number">4</span>,ensure_ascii=<span class="hljs-literal">False</span>)<br>            prompt += <span class="hljs-string">'&lt;|eot_id|&gt;\n'</span><br>    <span class="hljs-keyword">return</span> prompt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_response</span>(<span class="hljs-params">response</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(response,<span class="hljs-built_in">str</span>):<br>        response = json.loads(response)<br>    <span class="hljs-keyword">if</span> response[<span class="hljs-string">"role"</span>] == <span class="hljs-string">"assistant"</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">'&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n'</span> + response[<span class="hljs-string">"content"</span>] + <span class="hljs-string">'&lt;|eot_id|&gt;'</span><br>    <span class="hljs-keyword">else</span>:<br>        obj = response[<span class="hljs-string">"arguments"</span>]<br>        filtered_obj = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> obj.items() <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>}<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">'&lt;|start_header_id|&gt;search&lt;|end_header_id|&gt;\n'</span> + json.dumps(filtered_obj,indent=<span class="hljs-number">4</span>,ensure_ascii=<span class="hljs-literal">False</span>) + <span class="hljs-string">'&lt;|eot_id|&gt;'</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_json</span>(<span class="hljs-params">string</span>):<br>    search_pos = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># 开始寻找第一个 '{'</span><br>    start = string.find(<span class="hljs-string">'{'</span>, search_pos)<br>    <span class="hljs-keyword">if</span> start == -<span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    <span class="hljs-comment"># 从找到的 '{' 位置开始，向后寻找最后一个 '}'</span><br>    end = string.rfind(<span class="hljs-string">'}'</span>, start)<br>    <span class="hljs-keyword">if</span> end == -<span class="hljs-number">1</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    <span class="hljs-comment"># 提取并尝试解析 JSON</span><br>    json_string = string[start:end + <span class="hljs-number">1</span>]<br>    <span class="hljs-keyword">try</span>:<br>        obj = json.loads(json_string)<br>        <span class="hljs-keyword">return</span> obj<br>    <span class="hljs-keyword">except</span> json.JSONDecodeError:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>
<p><strong>模型加载</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (<br>    AutoTokenizer, <br>    AutoModelForCausalLM, <br>    BitsAndBytesConfig,<br>    DataCollatorForSeq2Seq, <br>    HfArgumentParser,<br>    TrainingArguments, <br>    Trainer<br>)<br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, TaskType, get_peft_model<br><span class="hljs-keyword">from</span> arguments <span class="hljs-keyword">import</span> ModelArguments, DataTrainingArguments, PeftArguments<br><span class="hljs-keyword">from</span> data_preprocess <span class="hljs-keyword">import</span> InputOutputDataset<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">model_name</span>):<br>    bnb_config = BitsAndBytesConfig(<br>        load_in_4bit=<span class="hljs-literal">True</span>,<br>        bnb_4bit_use_double_quant=<span class="hljs-literal">True</span>,<br>        bnb_4bit_quant_type=<span class="hljs-string">"nf4"</span>,<br>        bnb_4bit_compute_dtype=torch.float16,<br>    )<br>    n_gpus = torch.cuda.device_count()<br>    model = AutoModelForCausalLM.from_pretrained(<br>        model_name,<br>        quantization_config=bnb_config,<br>        device_map=<span class="hljs-string">"auto"</span>, <span class="hljs-comment"># dispatch efficiently the model on the available ressources</span><br>        max_memory = {i: <span class="hljs-string">'24500MB'</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_gpus)},<br>    )<br>    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=<span class="hljs-literal">True</span>)<br>    tokenizer.pad_token = tokenizer.eos_token<br>    <span class="hljs-keyword">return</span> model, tokenizer<br></code></pre></td></tr></table></figure>
<p><strong>LoRA主流程</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, PeftArguments, TrainingArguments))<br>    model_args, data_args, peft_args, training_args = parser.parse_args_into_dataclasses()<br><br>    lora_config = LoraConfig(<br>        inference_mode=<span class="hljs-literal">False</span>,<br>        task_type=TaskType.CAUSAL_LM, <br>        target_modules=[<span class="hljs-string">"q_proj"</span>, <span class="hljs-string">"k_proj"</span>, <span class="hljs-string">"v_proj"</span>],<span class="hljs-comment">#Q,K,V都进行微调</span><br>        r=peft_args.lora_rank, <br>        lora_alpha=peft_args.lora_alpha, <br>        lora_dropout=peft_args.lora_dropout<br>    )<br><br>    model, tokenizer  = load_model(model_args.model_name_or_path)<br>    model = get_peft_model(model, lora_config)<br>    model.print_trainable_parameters()<br><br>    data_collator = DataCollatorForSeq2Seq(<br>        tokenizer=tokenizer, <br>        padding=<span class="hljs-literal">True</span><br>    )<br><br>    <span class="hljs-keyword">if</span> training_args.do_train:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(data_args.train_file, <span class="hljs-string">"r"</span>, encoding=<span class="hljs-string">"utf-8"</span>) <span class="hljs-keyword">as</span> f:<br>            train_data = [json.loads(line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f]<br>        train_dataset = InputOutputDataset(train_data, tokenizer, data_args)<br>    <span class="hljs-keyword">if</span> training_args.do_eval:<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(data_args.validation_file, <span class="hljs-string">"r"</span>, encoding=<span class="hljs-string">"utf-8"</span>) <span class="hljs-keyword">as</span> f:<br>            eval_data = [json.loads(line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f]<br>        eval_dataset = InputOutputDataset(eval_data, tokenizer, data_args)<br><br>    trainer = Trainer(<br>        model=model,<br>        tokenizer=tokenizer,<br>        data_collator=data_collator,<br>        args=training_args,<br>        train_dataset=train_dataset <span class="hljs-keyword">if</span> training_args.do_train <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        eval_dataset=eval_dataset <span class="hljs-keyword">if</span> training_args.do_eval <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>    )<br><br>    <span class="hljs-keyword">if</span> training_args.do_train:<br>        model.gradient_checkpointing_enable()<br>        model.enable_input_require_grads()<br>        trainer.train()<br>    <span class="hljs-keyword">if</span> training_args.do_eval:<br>        trainer.evaluate()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:<br>    main()<br></code></pre></td></tr></table></figure>

    <div class="fold">
      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-0994800d" role="button" aria-expanded="false" aria-controls="collapse-0994800d">
        <div class="fold-arrow">▶</div>训练脚本
      </div>
      <div class="fold-collapse collapse" id="collapse-0994800d">
        <div class="fold-content">
          <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#! /usr/bin/env bash</span><br><br><span class="hljs-built_in">set</span> -ex<br><br>LR=2e-4<br><br>DATESTR=`<span class="hljs-built_in">date</span> +%Y%m%d-%H%M%S`<br>RUN_NAME=hotel_qlora<br>OUTPUT_DIR=output/<span class="hljs-variable">${RUN_NAME}</span>-<span class="hljs-variable">${DATESTR}</span><br><span class="hljs-built_in">mkdir</span> -p <span class="hljs-variable">$OUTPUT_DIR</span><br><br>MODEL_PATH=<span class="hljs-string">"/pathto/Meta-Llama-3-8B-Instruct"</span><br><br>CUDA_VISIBLE_DEVICES=0 python main_qlora.py \<br>    --do_train \<br>    --do_eval \<br>    --train_file ../data/train.llama3.jsonl \<br>    --validation_file ../data/dev.llama3.jsonl \<br>    --prompt_column context \<br>    --response_column response \<br>    --model_name_or_path <span class="hljs-string">"<span class="hljs-variable">${MODEL_PATH}</span>"</span> \<br>    --output_dir <span class="hljs-variable">$OUTPUT_DIR</span> \<br>    --max_source_length 2048 \<br>    --max_target_length 1024 \<br>    --per_device_train_batch_size 1 \<br>    --per_device_eval_batch_size 1 \<br>    --gradient_accumulation_steps 4 \<br>    --evaluation_strategy steps \<br>    --eval_steps 300 \<br>    --num_train_epochs 2 \<br>    --logging_steps 300 \<br>    --logging_dir <span class="hljs-variable">$OUTPUT_DIR</span>/logs \<br>    --save_steps 300 \<br>    --learning_rate <span class="hljs-variable">$LR</span> \<br>    --lora_rank 16 \<br>    --lora_alpha 32 \<br>    --lora_dropout 0.1 \<br>    --optim <span class="hljs-string">"paged_adamw_8bit"</span> \<br>    --warmup_ratio 0.1 \<br>    --fp16 2&gt;&amp;1 | <span class="hljs-built_in">tee</span> <span class="hljs-variable">${OUTPUT_DIR}</span>/train.log<br><br></code></pre></td></tr></table></figure>
        </div>
      </div>
    </div>
<h2 id="工具">工具</h2>
<p>HuggingFace 官方推出一个<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/hf-accelerate/model-memory-usage">在线工具</a>，可以估算模型的显存使用情况。</p>
<h2 id="参考">参考</h2>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://blog.philip-huang.tech/?page=peft-overview">概覽
Parameter-Efficient Fine-Tuning (PEFT)</a></li>
<li><a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/9138">Ladder
Side-Tuning：预训练模型的“过墙梯”</a></li>
<li><a target="_blank" rel="noopener" href="https://www.leewayhertz.com/parameter-efficient-fine-tuning/">Parameter-efficient
Fine-tuning (PEFT): Overview, benefits, techniques and model
training</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/peft">PEFT：在低资源硬件上对十亿规模模型进行参数高效微调</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/2301_77818837/article/details/135355289">参数高效微调方法（Parameter-Efficient
Fine-Tuning，PEFT）概述</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Stanlei/p/18096895">大模型参数高效微调技术原理综述</a></li>
<li><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters">Finetuning
LLMs Efficiently with Adapters</a></li>
<li><a target="_blank" rel="noopener" href="https://cwpeng.cn/p/llm%E5%BE%AE%E8%B0%83%E7%B3%BB%E5%88%971-adapter/">LLM微调系列1:
Adapter</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/hf-bitsandbytes-integration">A
Gentle Introduction to 8-bit Matrix Multiplication for transformers at
scale using Hugging Face Transformers, Accelerate and
bitsandbytes</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/LLMUZI123456789/article/details/136880839">peft模型微调_IA3</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.00751">Parameter-Efficient
Transfer Learning for NLP</a></li>
<li><a target="_blank" rel="noopener" href="https://www.linkedin.com/pulse/get-insight-from-your-business-data-build-llm-application-jain-2f">Get
Insight from your Business Data - Build LLM application with PEFT (with
LoRA) using 🤗 Hugging Face</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.04366">TOWARDS A UNIFIED VIEW OF
PARAMETER-EFFICIENT TRANSFER LEARNING</a></li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/LLM/" class="category-chain-item">LLM</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
        <a href="/tags/PEFT/" class="print-no-link">#PEFT</a>
      
        <a href="/tags/Fine-tuning/" class="print-no-link">#Fine-tuning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>LLM(七)——参数高效微调（Parameter-efficient fine-tuning,PEFT）</div>
      <div>https://mztchaoqun.com.cn/posts/D45_Fine-tuning/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>mztchaoqun</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年11月2日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-cc-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/D46_MoE/" title="LLM(八)——MoE">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">LLM(八)——MoE</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/D44_Position-Encoding/" title="LLM(六)——Position Encoding">
                        <span class="hidden-mobile">LLM(六)——Position Encoding</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <!-- <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> --> <div class="flex flex-auto justify-center [&amp;>*]:px-[16px] [&amp;>a]:no-underline  mb-[8px]"><a target="_blank" class="flex items-center text-[#A1A1A1] hover:text-white " href="https://www.beian.gov.cn/portal/registerSystemInfo?recordcode=51015602000856"><img alt="川公网安备" fetchpriority="high" width="20" height="20" decoding="async" data-nimg="1" class="mr-[6px]" src="/images/ga.png" srcset="/img/loading.gif" lazyload style="color: transparent;">&nbsp;川公网安备&nbsp;51015602000856号</a>&emsp;<a target="_blank" class="text-[#A1A1A1] hover:text-white " href="https://beian.miit.gov.cn/">蜀ICP备2024061486号-1</a></div> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
